{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_e91Czih9AEU"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds \n",
        "import pandas as pd\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RZ_ikSg6isr4"
      },
      "outputs": [],
      "source": [
        "def _split_into_words(sentences):\n",
        "    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "\n",
        "def _get_word_ngrams(n, sentences, exclusive=True):\n",
        "    \"\"\"Calculates word n-grams for multiple sentences\"\"\"\n",
        "    \n",
        "    assert len(sentences) > 0\n",
        "    assert n > 0\n",
        "\n",
        "    words = _split_into_words(sentences)\n",
        "    return _get_ngrams(n, words)\n",
        "\n",
        "def _get_ngrams(n, text):\n",
        "    \"\"\"Calculates n-grams.\n",
        "    Args:\n",
        "      n: which n-grams to calculate\n",
        "      text: A list of tokens\n",
        "    Returns:\n",
        "      A set of n-grams \"\"\"\n",
        "\n",
        "    n_grams = set()\n",
        "\n",
        "    # Loop through words, creating n-grams\n",
        "    for i in range((len(text)) - n + 1):\n",
        "        ngram = \" \".join(text[i:i+n])\n",
        "        n_grams.add(ngram)\n",
        "\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JsRtwSrShyaH"
      },
      "outputs": [],
      "source": [
        "def rouge_n(evaluated_sentences, reference_sentences,\n",
        "            n=2, raw_results=False, exclusive=True):\n",
        "    \"\"\"\n",
        "    Computes ROUGE-N of two text collections of sentences.\n",
        "    Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
        "    papers/rouge-working-note-v1.3.1.pdf\n",
        "    Args:\n",
        "      evaluated_sentences: The sentences that have been picked by the\n",
        "                           summarizer\n",
        "      reference_sentences: The sentences from the referene set\n",
        "      n: Size of ngram.  Defaults to 2.\n",
        "    Returns:\n",
        "      A tuple (f1, precision, recall) for ROUGE-N\n",
        "    Raises:\n",
        "      ValueError: raises exception if a param has len <= 0\n",
        "    \"\"\"\n",
        "    if len(evaluated_sentences) <= 0:\n",
        "        raise ValueError(\"Hypothesis is empty.\")\n",
        "    if len(reference_sentences) <= 0:\n",
        "        raise ValueError(\"Reference is empty.\")\n",
        "\n",
        "    evaluated_ngrams = _get_word_ngrams(\n",
        "        n, evaluated_sentences, exclusive=exclusive)\n",
        "    reference_ngrams = _get_word_ngrams(\n",
        "        n, reference_sentences, exclusive=exclusive)\n",
        "    reference_count = len(reference_ngrams)\n",
        "    evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "    # Gets the overlapping ngrams between evaluated and reference\n",
        "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "    overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "    if raw_results:\n",
        "        o = {\n",
        "            \"hyp\": evaluated_count,\n",
        "            \"ref\": reference_count,\n",
        "            \"overlap\": overlapping_count\n",
        "        }\n",
        "        return o\n",
        "    else:\n",
        "        return f_r_p_rouge_n(\n",
        "            evaluated_count, reference_count, overlapping_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HuxNBtBFTmf7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Clustering:\n",
        "    \"\"\"This class is used to cluster sentence embeddings in order to execute\n",
        "    text summarization. \n",
        "    Args:\n",
        "    - features (np.ndarray): sentence embeddings\n",
        "    - random_state (int - optional): random state for random seed\n",
        "    \"\"\"\n",
        "\n",
        "    features: np.ndarray\n",
        "    random_state: int = 1\n",
        "\n",
        "    def __define_model(self, k: int) -> None:\n",
        "        \"\"\"used to define KNN clustering model\"\"\"\n",
        "\n",
        "        model = KMeans(n_clusters=k, random_state=self.random_state)\n",
        "        object.__setattr__(self, 'model', model)\n",
        "\n",
        "    def __find_closest_sents(self, centroids: np.ndarray) -> Dict:\n",
        "        \"\"\"\n",
        "        Find the closest arguments to centroid.\n",
        "        - centroids: Centroids to find closest.\n",
        "        - return: Closest arguments.\n",
        "        \"\"\"\n",
        "\n",
        "        centroid_min = 1e10\n",
        "        cur_arg = -1\n",
        "        args = {}\n",
        "        used_idx = []\n",
        "\n",
        "        for j, centroid in enumerate(centroids):\n",
        "\n",
        "            for i, feature in enumerate(self.features):\n",
        "                value = np.linalg.norm(feature - centroid)\n",
        "\n",
        "                if value < centroid_min and i not in used_idx:\n",
        "                    cur_arg = i\n",
        "                    centroid_min = value\n",
        "\n",
        "            used_idx.append(cur_arg)\n",
        "            args[j] = cur_arg\n",
        "            centroid_min = 1e10\n",
        "            cur_arg = -1\n",
        "\n",
        "        return args\n",
        "\n",
        "    def cluster(self, ratio: float = 0.2,\n",
        "                num_sentences: int = None) -> List[int]:\n",
        "        \"\"\"\n",
        "        Clusters sentences based on the ratio.\n",
        "        - ratio: Ratio to use for clustering.\n",
        "        - num_sentences: Number of sentences. Overrides ratio.\n",
        "        return: Sentences index that qualify for summary.\n",
        "        \"\"\"\n",
        "\n",
        "        # set k value\n",
        "        if num_sentences is not None:\n",
        "            if num_sentences == 0:\n",
        "                return []\n",
        "            k = min(num_sentences, len(self.features))\n",
        "        else:\n",
        "            k = max(int(len(self.features) * ratio), 1)\n",
        "\n",
        "        # define n train the model\n",
        "        self.__define_model(k)\n",
        "        self.model.fit(self.features)\n",
        "\n",
        "        # find the closest embeddings to the center\n",
        "        centroids = self.model.cluster_centers_\n",
        "        cluster_args = self.__find_closest_sents(centroids)\n",
        "\n",
        "        sorted_values = sorted(cluster_args.values())\n",
        "        return sorted_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z3LllBetUC6P"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from typing import Union\n",
        "\n",
        "REGEX_URL = r'((http|https)\\:\\/\\/)[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*'\n",
        "clear_url = lambda text: re.sub(REGEX_URL, ' ', text)\n",
        "DOT_REGEX = r\"(?<!\\w)(?:[A-Z][A-Za-z]{,3}|[a-z]{1,2})\\.\"\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Preprocessing:\n",
        "    \"\"\"Preprocessing class used to preprocess news text before Text\n",
        "    Summarization is applied. \"\"\"\n",
        "\n",
        "    def _clear_content_head(self, content: str, site_name: str,\n",
        "                           head_pattern: str=r\"\\s\\-+\\s\") -> str:\n",
        "        \"\"\"used to clear any head in given news content\"\"\"\n",
        "\n",
        "        match = re.search(head_pattern, content)\n",
        "        if match:\n",
        "            idx_end = match.end()\n",
        "            site_name = site_name.split()[0]\n",
        "            if site_name.lower() in content[:idx_end].lower():\n",
        "                content = content[idx_end:]\n",
        "\n",
        "        return content\n",
        "\n",
        "    def _clear_abbreviation_dot(self, text: str) -> str:\n",
        "        \"\"\"used to rip off abbreviation dot in given text\"\"\"\n",
        "\n",
        "        # replace any matched abbr with empty string\n",
        "        text_list = list(text)\n",
        "        for i, match in enumerate(re.finditer(DOT_REGEX, text)):\n",
        "            no_dot = match.group().replace('.', '')\n",
        "            idx = match.span()\n",
        "            text_list[idx[0]-i: idx[1]-i] = no_dot\n",
        "\n",
        "        # join list of texts and clear multiple whitespaces\n",
        "        text = ''.join(text_list)\n",
        "        text = re.sub(' +', ' ', text)\n",
        "\n",
        "    def __call__(self, content: str, site_name: str) -> Union[str, bool]:\n",
        "\n",
        "        \"\"\"the method is used to:\n",
        "        - clear any content head\n",
        "        - clear any heading/tailing whitespace & punct\n",
        "        - clear any abbreviation dot\n",
        "\n",
        "        Args:\n",
        "        - content (str): news content\n",
        "        - site_name (str): news site name\n",
        "\n",
        "        Return:\n",
        "        preprocessed content\n",
        "        \"\"\"\n",
        "\n",
        "        content = self._clear_content_head(content, site_name)\n",
        "        content = clear_url(content)\n",
        "\n",
        "        # clear leadding/trailing whitespaces & puncts\n",
        "        content = content.strip(string.punctuation)\n",
        "        content = content.strip()\n",
        "\n",
        "        # change multiple whitespaces to single one\n",
        "        content = re.sub(' +', ' ', content)\n",
        "\n",
        "        # clear whitespace before dot\n",
        "        content = re.sub(r'\\s+([?,.!\"])', r'\\1', content)\n",
        "\n",
        "        return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "--kM7K6JUH7R"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from dataclasses import dataclass\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Word2VecSummarizer:\n",
        "    \"\"\"The main class for Word2Vec Summarizer\n",
        "\n",
        "    Args:\n",
        "    - model: A gensim Word2Vec model (optional)\n",
        "    - random_state: state for random seed (optional)\n",
        "    \"\"\"\n",
        "    def __init__(self, model: Word2Vec, random_state: int=1):\n",
        "        object.__setattr__(self, 'model', model)\n",
        "        object.__setattr__(self, 'random_state', random_state)\n",
        "\n",
        "    def __split_sentence(self, text: str) -> List[str]:\n",
        "        \"\"\"used to split given text into sentences\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        return [sent for sent in sentences if len(sent) >= 5]\n",
        "\n",
        "    def __set_embedder(self) -> None:\n",
        "        \"\"\"used to instantiate Embedder object\"\"\"\n",
        "        embedder = Embedder(self.model)\n",
        "        object.__setattr__(self, 'embedder', embedder)\n",
        "\n",
        "    def __set_clusterer(self, features: np.ndarray,\n",
        "                        random_state: int) -> None:\n",
        "        \"\"\"used to instantiate Clustering object\"\"\"\n",
        "        clusterer = Clustering(features, random_state)\n",
        "        object.__setattr__(self, 'clusterer', clusterer)\n",
        "\n",
        "    def summarize(self, text: str,\n",
        "                  use_first: bool = True,\n",
        "                  num_sentences: int = None,\n",
        "                  ratio: float = 0.2,\n",
        "                  return_oov: bool = False) -> Tuple[List[str], np.ndarray]:\n",
        "        \n",
        "        \"\"\"This method executes the summarization part. It returns a tuple of sentences and related embeddings (and OOV list if return_oov set to True)\"\"\"\n",
        "        \n",
        "        list_sentence = self.__split_sentence(text)\n",
        "        self.__set_embedder()\n",
        "\n",
        "        # set buffers\n",
        "        sent_vecs = []\n",
        "        oov_list = []\n",
        "\n",
        "        # loop through each sentence to create each embeddings\n",
        "        for sentence in list_sentence:\n",
        "            if return_oov:\n",
        "                vec, oov = self.embedder.embed(sentence, return_oov)\n",
        "                oov_list.extend(oov)\n",
        "            else:\n",
        "                vec = self.embedder.embed(sentence, return_oov)\n",
        "\n",
        "            # check if no OOV returned\n",
        "            if isinstance(vec, np.ndarray):\n",
        "                sent_vecs.append(vec)\n",
        "\n",
        "        sent_vecs = np.array(sent_vecs) # create array of all embeddings\n",
        "\n",
        "        # instantiate clustering & process\n",
        "        self.__set_clusterer(sent_vecs, self.random_state)\n",
        "        summary_idx = self.clusterer.cluster(ratio, num_sentences)\n",
        "\n",
        "        if use_first:\n",
        "            if not summary_idx:\n",
        "                summary_idx.append(0)\n",
        "\n",
        "            elif summary_idx[0] != 0:\n",
        "                summary_idx.insert(0, 0)\n",
        "\n",
        "        sentences = [list_sentence[idx] for idx in summary_idx]\n",
        "        embeddings = np.asarray([sent_vecs[idx] for idx in summary_idx])\n",
        "\n",
        "        if return_oov:\n",
        "            return sentences, oov_list\n",
        "        return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ot4BwK6GUQa1"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Embedder:\n",
        "    \"\"\"This class is used to create word embeddings from given sentence. It converts each token of given sentence \n",
        "        to its representative vector then calculates the mean of all tokens in given sentence in order to get a\n",
        "    sentence embedding.\n",
        "\n",
        "    Arg:\n",
        "    - model: a gensim Word2Vec model\n",
        "    \"\"\"\n",
        "\n",
        "    model: Word2Vec\n",
        "\n",
        "    def __get_vector(self, token: str) -> np.ndarray:\n",
        "        \"\"\"used to convert given token to its representative vector\"\"\"\n",
        "        try:\n",
        "            return self.model.wv.get_vector(token)\n",
        "        except KeyError:\n",
        "            return False\n",
        "\n",
        "    def __averaging(self, token_matrix: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"used to calculate mean of an array of vectors in order to get a\n",
        "        sentence embedding\"\"\"\n",
        "        return np.mean(token_matrix, axis=0)\n",
        "\n",
        "    def embed(self, sentence: str, return_oov: bool=False) -> np.ndarray:\n",
        "        \"\"\"combine all other methods to execute the embedding process.\n",
        "        \n",
        "        Args:\n",
        "        - sentence (str): a sentence to be process to get its embedding\n",
        "        - return_oov(bool): indicate if you'd like to return the OOV\n",
        "        (out-of-vocabulary) tokens\n",
        "        \n",
        "        Returns:\n",
        "        If all tokens in given sentence are OOV tokens, return False (and with\n",
        "        list of OOVs if 'return_oov' set to True).\n",
        "        else, return the sentence embedding (and with list of OOVs if\n",
        "        'return_oov' set to True).\n",
        "        \"\"\"\n",
        "\n",
        "        # make the given sentence lowercase and collect only words\n",
        "        list_tok = re.findall(r\"\\w+\", sentence.lower())\n",
        "\n",
        "        # buffers\n",
        "        list_vec = []\n",
        "        OOV_tokens = []\n",
        "\n",
        "        # loop through each token of given sentence\n",
        "        for token in list_tok:\n",
        "            tokvec = self.__get_vector(token) # convert to vector\n",
        "\n",
        "            # check if no OOV token produced\n",
        "            if isinstance(tokvec, np.ndarray):\n",
        "                list_vec.append(tokvec)\n",
        "            else:\n",
        "                OOV_tokens.append(token)\n",
        "\n",
        "        # if all tokens in given sentence are OOV tokens\n",
        "        if not list_vec:\n",
        "            if return_oov:\n",
        "                return False, OOV_tokens\n",
        "            return False\n",
        "\n",
        "        # if not\n",
        "        list_vec = np.array(list_vec)\n",
        "        if return_oov:\n",
        "            return (self.__averaging(list_vec), OOV_tokens)\n",
        "        return self.__averaging(list_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4vnoDUk0ZEPo"
      },
      "outputs": [],
      "source": [
        "def f_r_p_rouge_n(evaluated_count, reference_count, overlapping_count):\n",
        "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
        "    if evaluated_count == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = overlapping_count / evaluated_count\n",
        "\n",
        "    if reference_count == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = overlapping_count / reference_count\n",
        "\n",
        "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgMjtxrt89EZ",
        "outputId": "1f5dff22-0030-4b93-ff65-d80ff02a9861"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'f': 0.08834916855171028, 'p': 0.1426947129050597, 'r': 0.06398164669895488}\n",
            "{'f': 0.09701042058635606, 'p': 0.1645717626425022, 'r': 0.06877595895431224}\n",
            "{'f': 0.10624169577534219, 'p': 0.18514791708593278, 'r': 0.07449392712550608}\n",
            "{'f': 0.10916607611810407, 'p': 0.18906154783668494, 'r': 0.07673757111056147}\n",
            "{'f': 0.1098807455115188, 'p': 0.19379068602904356, 'r': 0.07667921537547058}\n",
            "{'f': 0.1102476890213645, 'p': 0.19674972914409533, 'r': 0.07657923589440836}\n",
            "{'f': 0.11230129987770805, 'p': 0.1995113241236726, 'r': 0.07814340400471143}\n",
            "{'f': 0.11665155227415015, 'p': 0.2036691289877425, 'r': 0.08173170890611767}\n",
            "{'f': 0.11802977444653152, 'p': 0.20574336604870955, 'r': 0.08275095763034007}\n",
            "{'f': 0.12152836452022045, 'p': 0.21342718640015937, 'r': 0.08495004493312894}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import tensorflow_datasets as tfds \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "summaries = []\n",
        "vals_highs = []\n",
        "fpr = tpr = []\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "data = list(tfds.load('cnn_dailymail', split='all', as_supervised=True)) #CNN Daily Mail dataset\n",
        "data = data[:1000] #Past a certain size, the notebook stopped working so must experiment with different sizes\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "    \n",
        "for train, test in kfold.split(data):\n",
        "\n",
        "  data_test = [data[i] for i in test]\n",
        "  data_train = [data[i] for i in train]\n",
        "\n",
        "  vals_highlights  = [data_test[i][1] for i in range(len(data_test))]\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(data_train)): \n",
        "    data_train[i] = preprocessor(data[i][0].numpy().decode('utf-8'), 'Daily Mail' )\n",
        "    data_train[i] = word_tokenize(data[i][0].numpy().decode('utf-8')) #tokenizes each training article\n",
        "    \n",
        "    \n",
        "  summarizes = Word2VecSummarizer(Word2Vec(data_train))\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(data_test)):\n",
        "    clean_text = preprocessor(data_test[i][0].numpy().decode('utf-8'), 'Daily Mail' )\n",
        "    summaries.append(summarizes.summarize(clean_text)[0])\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(len(vals_highlights)):\n",
        "    vals_highlights[i] = vals_highlights[i].numpy().decode('utf-8')\n",
        "    vals_highs.append(vals_highlights[i])\n",
        "\n",
        "  scores = rouge_n(summaries, vals_highs)\n",
        "  \n",
        "  tpr.append(scores['r'])\n",
        "  FP_Rate = (1-scores['p']) / ((1-scores['p']) + scores['r'] / scores['f'])\n",
        "  fpr.append(FP_Rate)\n",
        "\n",
        "  print(scores) #recall is true positive; FPR = (1 - precision) / ((1 - precision) + recall / F1)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
